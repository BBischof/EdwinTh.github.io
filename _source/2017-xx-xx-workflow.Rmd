---
layout: post
categories: blog
title: "A two-stage workflow for data science projects"
base-url: https://EdwinTh.github.io
date: "2017-"
output: html_document
tags: [R, data science, software engineering, workflow]
---

If you are a data scientist who primarily uses R, chances are you had no formal training in software development. I certainly did not pick up much skills in that direction during my statistics masters. For years my workflow was basically loading dataset and hack away on them. In the best case these scripts came to some conclusion or final data set. Usually they abruptly ended. Complex projects could result in a great number of scripts and data exports. Needless to say that reducability was low and stress could get high when some delivery went wrong.

## Picking up some software skills

Colleagues pointed me to some customs in software design, such as creating functions and classes (and documenting them), writing unit tests, and packaging your work. I started to read about these and was soon working my way through Hadley's [R packages](http://r-pkgs.had.co.nz/). I came to a adopt a rigorous system of solely writing functions, and thoroughly documenting and unit testing them. No matter the nature of the project I was working on, I treated is as a package that should be published on CRAN. This slowed me down considerably though. I needed increasing amounts of time to answer relatively simple questions. Maybe now I was overengineering, restricting myself in such a way I was losing productivity.

## Data science product

In my opinion, the tension between efficiency and rigour is that the data scientist's product usually is not software, but insight. This goal is not always served best by writing code that meets the highest standards, because this can make an insight more expensive than necesarry, timewise. A software engineer's job is much more clear cut. His product is software, and the better his software, the better he does his job. However, when a data scientist is asked if she can deliver an exploratory analysis on a new data set, should she create a full software stack right away? I would say not. Of course clean, well documented scripts will lead to greater reproducibility of and confidence in her results. However, writing solely functions and classes and put a load of unit tests is overkill in this stage.

## A dual mode

I came to a dual workflow, with which I am quite happy. It has a formal split between exploratory stage and a product stage.

### Exploratory stage

Starting an analysis, I want to get to first results as quickly as possible. I typically work in R markdown files, in which I take a lot of notes about the quality of the data, assumptions tested, relations found, and things that are not yet clear to me. Although being still exploratory, I already create a lot of functions in this stage. Functions to do transformations, create specific plots and test specific relationships. These functions are already generic. It is my experience that data is often refreshed, even when this is not anticipated at the start of the project. I don't hardcode names of dataframes and columns but give them as arguments to functions with the current names as their default values. These functions can be quickly lifted to the product stage. Typically, the exploratory stage is wrapped-up by a report in which the major insights and next-steps are described. More often than not, the exporatory stage is the only part of a project. Often we have to conclude that the project goal cannot be reached (yet). The quicker we can get to such a conclusion, the less resources are wasted.

### Product stage

A product, for me, is everything that is built to share insights and data with others. When a product is built, whether this is a Shiny app, an export of model productions, or a daily updated report, the product stage is entered. In more ignorant days I used gradually slide from exploratory into product stage. In the worst cases commenting out the parts of code that were not needed for building the product. This got increasingly messy of course, yielding nonreproducable results and projects that grinded to a halt. Nowadays, as soon as a product is built I completely start over. Here the software rigor begins, all code written in this part only serves the final product, no explorations or asides. I use R's package structure, writing functions that work together to create the product, including documentation and unit tests. Since I used functions in the exploratory stage, I can often quickly build the components for the product.

## Further cycles

When during product stage it becomes evident that further research is needed, I switch back to the exploratory code base. I think it is crucial to be very string about this. No mess in the product repository! The product directory only contains the code needed for the product to work. When, subsequently, new insight requires the product to change I adjust the product. Again only code necesarry for the product to function. Legacy code no longer needed should be removed, not be commented out. This workflow prevents a large set of scripts with a great number of models, many exported data sets, several versions of a Shiny app etc. 

I hope these hard-learned lessons might be to some value for some of you. I am very curious what others designed as their system. Please share it in the comments of this blog, on Twitter, or in you own blog. 

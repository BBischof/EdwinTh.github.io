---
layout: post
categories: blog
title: "A two-stage workflow for data science projects"
base-url: https://EdwinTh.github.io
date: "2017-"
output: html_document
tags: [R, data science, software engineering, workflow]
---

How rigorous should your analysis workflow be? I am struggling with this question for some time now. After working my way through Hadley's [R packages](http://r-pkgs.had.co.nz/) and a couple of other resources about two years ago, I left the state of blessful ignorance about software standards. Trained as a statistician I used to just create a lot of R scripts and data exports containing intermediate results in projects. As my R and programming skills went up it became increasingly clear why more complex projects ended in irreproducability and stress. I came to a adopt a rigorous system of solely writing functions, documenting and unit testing them. As if every project I ran had to result in a CRAN package. This slowed me down considerably though. I needed increasing amounts of time to answer relatively simple questions. Maybe now I was overengineering, restricting myself in such a way I was losing productivity.

## Data Science product

In my opinion, the tension between efficiency and rigour is that the data scientist's product usually is insight instead of software. This goal is not always served best by writing code that meets the highest standards, because this can make an insight more expensive than necesarry, timewise. A software engineer's job is much more clear cut. His product is software, and the better his software, the better he does his job. However, when a data scientist is asked if she can deliver an exploratory analysis on a new data set, should she create a full software stack right away? I would say not. She should do her job rigorously and make it reproducible, but it can be completely tailored to the problem at hand. 

## A dual mode

I came to a dual workflow, with which I am quite happy. With a formal split between exploratory stage and a product stage.

### Exploratory stage

Starting an analysis, I want to get to results as quickly as possible. I typically work in R markdown files, in which I take a lot of notes about assumptions tested, relations found, and things that are not yet clear to me. Although being still exploratory, I already create a lot of functions in this stage. Functions to do transformations, create specific plots and test specific relationships. These functions are already quite generic. It is my experience that data is often refreshed, even when this is not foreseen at the start of the project. I don't hardcode names of dataframes and columns but give them as arguments to functions with the current names as their default values. These functions can be quickly lifted to the product stage. Typically, the exploratory stage is wrapped-up by a report in which the major insights and next-steps are described.

### Product stage

More often than not, the exporatory stage is the only part of a project. As mentioned, in my experience insight is the data scientist's main product, no matter how much hyped articles about AI are written. This of course, can be different from job to job. When a product is built, whether this is a Shiny app, an export of model productions, or a daily updated report, the product stage is entered. In ignorant days I used gradually slide from exploratory to product stage. In the worst cases commenting out the parts of code that were not needed for building the product. This got increasingly messy of course, yielding nonproducable products and projects that grinded to a halt. Nowadays, as soon as a product is built I start over. Here the software rigor begins, all code written in this part only serves the final product. I use R's package structure, writing functions that work together to create the product, including documentation and unit tests. Using functions in the exploratory stage enables me to quickly build the components for the product.

## Further cycles

When during product stage it becomes evident that further research is needed, I switch back to the exploratory code base. I think it is crucial to be very string about this. The product directory only contains the code needed for the product to work. When new insight requires the product to change I adjust the product. This workflow prevents a large set of scripts with a great number of models, many exported data sets, several versions of a Shiny app etc. It is always clear what the current version of the product


